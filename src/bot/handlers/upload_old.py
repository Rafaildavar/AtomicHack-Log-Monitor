"""–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–æ–≤."""

import asyncio
import logging
import os
import zipfile
from typing import Dict

import pandas as pd
from aiogram import Router, F
from aiogram.filters import StateFilter
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.types import Message, Document, FSInputFile

from ..keyboards.main import build_main_menu_keyboard
from ..services.log_parser import LogParser
from ..services.analysis_history import SimpleFileAnalyzer

logger = logging.getLogger(__name__)

router = Router(name="upload")
file_analyzer = SimpleFileAnalyzer()
log_parser = LogParser()


class UploadStates(StatesGroup):
    """–°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤."""
    waiting_for_file = State()


@router.message(F.text == "–ó–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–≥–∏")
async def start_upload_process(message: Message, state: FSMContext) -> None:
    """–ù–∞—á–∏–Ω–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤."""

    await message.answer(
        "üìÅ –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏ (txt, log –∏–ª–∏ zip –∞—Ä—Ö–∏–≤).\n"
        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 20MB."
    )
    await state.set_state(UploadStates.waiting_for_file)


@router.message(UploadStates.waiting_for_file, F.document)
async def handle_file_upload(message: Message, state: FSMContext) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏."""

    document = message.document

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if document.file_size and document.file_size > 20 * 1024 * 1024:
        await message.answer(
            "‚ùå –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 20MB).\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∂–∞—Ç—å —Ñ–∞–π–ª –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –º–µ–Ω—å—à–∏–π."
        )
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
    allowed_extensions = ['.txt', '.log', '.zip']
    file_name = document.file_name.lower() if document.file_name else ""

    if not any(file_name.endswith(ext) for ext in allowed_extensions):
        await message.answer(
            "‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã .txt, .log –∏–ª–∏ .zip –∞—Ä—Ö–∏–≤—ã.\n"
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."
        )
        return

    await message.answer("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ñ–∞–π–ª... –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ.")

    try:
        # –®–ê–ì 1: –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤—Ä—É—á–Ω—É—é –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º
        # –í–ê–ñ–ù–û: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º process_uploaded_file, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –æ—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        # –°–Ω–∞—á–∞–ª–∞ —Å–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤—Ä—É—á–Ω—É—é, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Ñ–∞–π–ª–∞–º
        file_path = await log_parser._download_file(document, message.bot)

        # –®–ê–ì 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if file_path.endswith('.zip'):
            logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ ZIP —Ñ–∞–π–ª–∞")
            log_files = await log_parser._extract_zip(file_path)
            logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(log_files)} —Ñ–∞–π–ª–æ–≤")
        else:
            log_files = [file_path]
            logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")

        # –®–ê–ì 3: –ü–∞—Ä—Å–∏–º –ª–æ–≥–∏ –≤ DataFrame —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        df = await log_parser._parse_logs_to_dataframe(log_files)
        logs_df = df

        if logs_df.empty:
            await message.answer("‚ùå –í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ª–æ–≥–æ–≤ –≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.")
            await state.clear()
            return

        # –®–ê–ì 4: –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ WARNING –∏ ERROR (–∫–∞–∫ –≤ log_to_df.py)
        filtered_logs = anomaly_analyzer.filter_warnings_and_errors(logs_df)

        if filtered_logs.empty:
            await message.answer("‚ùå –í —Ñ–∞–π–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å WARNING –∏–ª–∏ ERROR.")
            await state.clear()
            return

        # –®–ê–ì 5: –ò—â–µ–º –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ –∞–Ω–æ–º–∞–ª–∏–π –≤ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–Ω–æ–º –∞—Ä—Ö–∏–≤–µ –î–û –æ—á–∏—Å—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
        # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ - —Ñ–∞–π–ª—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ anomalies_problems.csv
        anomalies_files = []
        base_dir = None

        if log_files:
            first_file = log_files[0]
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ä–µ–π
            base_dir = log_parser.temp_manager.temp_dirs[0] if log_parser.temp_manager.temp_dirs else os.path.dirname(first_file)

        if base_dir:
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã anomalies_problems.csv
            for root, dirs, files in os.walk(base_dir):
                for file in files:
                    if file == 'anomalies_problems.csv':
                        anomalies_files.append(os.path.join(root, file))

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤–∞—Ä–µ–π –∞–Ω–æ–º–∞–ª–∏–π: {len(anomalies_files)}")

        if not anomalies_files:
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            log_parser.temp_manager.cleanup()
            await message.answer(
                "‚ö†Ô∏è –°–ª–æ–≤–∞—Ä–∏ –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∞—Ä—Ö–∏–≤–µ.\n"
                f"–ù–∞–π–¥–µ–Ω–æ {len(filtered_logs)} —Å—Ç—Ä–æ–∫ —Å WARNING/ERROR, –Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ."
            )
            await state.clear()
            return

        # –®–ê–ì 6: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –æ—Ç–¥–µ–ª—å–Ω–æ (–∫–∞–∫ –≤ data_analysis.ipynb)
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ª–æ–≥–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        all_results = []

        for anomalies_file in anomalies_files:
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π (—Ñ–æ—Ä–º–∞—Ç: ID –∞–Ω–æ–º–∞–ª–∏–∏, –ê–Ω–æ–º–∞–ª–∏—è, ID –ø—Ä–æ–±–ª–µ–º—ã, –ü—Ä–æ–±–ª–µ–º–∞)
                anomalies_dict = pd.read_csv(anomalies_file, sep=';')
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Å–ª–æ–≤–∞—Ä—å: {anomalies_file} ({len(anomalies_dict)} –∞–Ω–æ–º–∞–ª–∏–π)")

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞–ø–∫—É —Å—Ü–µ–Ω–∞—Ä–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, ValidationCase 1)
                scenario_dir = os.path.dirname(anomalies_file)
                scenario_name = os.path.basename(scenario_dir)

                # –§–∏–ª—å—Ç—Ä—É–µ–º –ª–æ–≥–∏ —Ç–æ–ª—å–∫–æ –∏–∑ —ç—Ç–æ–π –ø–∞–ø–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è
                # –ò—â–µ–º —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ —Å –∏–º–µ–Ω–µ–º —Å—Ü–µ–Ω–∞—Ä–∏—è
                scenario_logs = filtered_logs[filtered_logs['filename'].apply(
                    lambda x: scenario_name in x and os.path.basename(os.path.dirname(x)) == scenario_name
                )]

                if scenario_logs.empty:
                    logger.warning(f"–î–ª—è —Å—Ü–µ–Ω–∞—Ä–∏—è {scenario_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ª–æ–≥–æ–≤")
                    continue

                logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω–∞—Ä–∏—è {scenario_name}: {len(scenario_logs)} —Å—Ç—Ä–æ–∫ –ª–æ–≥–æ–≤")

                # –®–ê–ì 7: –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—ã –¥–ª—è —ç—Ç–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è (–∞–ª–≥–æ—Ä–∏—Ç–º –∏–∑ result_df.py)
                # –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞ –∏—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ —Å—Ç–æ–ª–±—Ü–µ "–ê–Ω–æ–º–∞–ª–∏—è"
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ - –±–µ—Ä–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é "–ü—Ä–æ–±–ª–µ–º—É" –∏ –∏—â–µ–º –µ—ë –≤ –ª–æ–≥–∞—Ö
                scenario_results = anomaly_analyzer.find_anomaly_problem_chain(scenario_logs, anomalies_dict)

                if not scenario_results.empty:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    scenario_results['–°—Ü–µ–Ω–∞—Ä–∏–π'] = scenario_name
                    all_results.append(scenario_results)
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(scenario_results)} –ø—Ä–æ–±–ª–µ–º –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ {scenario_name}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è {anomalies_file}: {e}")
                continue

        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        log_parser.temp_manager.cleanup()

        if not all_results:
            await message.answer(
                f"üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\n\n"
                f"–ù–∞–π–¥–µ–Ω–æ {len(filtered_logs)} —Å—Ç—Ä–æ–∫ —Å WARNING/ERROR,\n"
                f"–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –ø–æ —Å–ª–æ–≤–∞—Ä—è–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
            )
            await state.clear()
            return

        # –®–ê–ì 8: –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –≤ –æ–¥–∏–Ω DataFrame
        final_results = pd.concat(all_results, ignore_index=True)

        # –®–ê–ì 9: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–∞
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏–∏
        analysis_data = {
            'total_lines': len(logs_df),
            'error_count': len([row for row in logs_df.itertuples() if row.level.upper() in ['ERROR', 'CRITICAL', 'FATAL']]),
            'warning_count': len([row for row in logs_df.itertuples() if row.level.upper() == 'WARNING']),
            'info_count': len([row for row in logs_df.itertuples() if row.level.upper() == 'INFO']),
            'sources': logs_df['source'].value_counts().to_dict() if 'source' in logs_df.columns else {},
            'top_messages': logs_df['text'].value_counts().head(5).to_dict() if 'text' in logs_df.columns else {},
            'level_distribution': logs_df['level'].value_counts().to_dict() if 'level' in logs_df.columns else {}
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        try:
            logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ –∏—Å—Ç–æ—Ä–∏—é: —Ñ–∞–π–ª={document.file_name}, —Å—Ç—Ä–æ–∫={len(logs_df)}, –∞–Ω–æ–º–∞–ª–∏–π={len(final_results) if not final_results.empty else 0}")
            history_service.add_record(
                user_id=message.from_user.id,
                analysis=analysis_data,
                file_name=document.file_name or "unknown_file",
                anomaly_analysis=final_results
            )
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {message.from_user.id}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é: {e}", exc_info=True)

        # –®–ê–ì 10: –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Excel —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º –∫–æ–ª–æ–Ω–æ–∫
        excel_path = anomaly_analyzer.export_to_excel(final_results)

        if not excel_path:
            await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞.")
            await state.clear()
            return

        # –®–ê–ì 11: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º Excel —Ñ–∞–π–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        excel_file = FSInputFile(excel_path)

        await message.answer_document(
            excel_file,
            caption=f"üìä –ù–∞–π–¥–µ–Ω–æ {len(final_results)} –ø—Ä–æ–±–ª–µ–º –≤ {len(anomalies_files)} —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö"
        )

        await message.answer(
            f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\n\n"
            f"üìà –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(logs_df)}\n"
            f"‚ö†Ô∏è –°—Ç—Ä–æ–∫ —Å WARNING/ERROR: {len(filtered_logs)}\n"
            f"üìã –°—Ü–µ–Ω–∞—Ä–∏–µ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(anomalies_files)}\n"
            f"üö® –ò–∑–≤–µ—Å—Ç–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–∞–π–¥–µ–Ω–æ: {len(final_results)}\n\n"
            f"Excel —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç:\n"
            f"‚Ä¢ –°—Ü–µ–Ω–∞—Ä–∏–π\n"
            f"‚Ä¢ ID –∞–Ω–æ–º–∞–ª–∏–∏\n"
            f"‚Ä¢ ID –ø—Ä–æ–±–ª–µ–º—ã\n"
            f"‚Ä¢ –§–∞–π–ª —Å –ø—Ä–æ–±–ª–µ–º–æ–π\n"
            f"‚Ä¢ ‚Ññ —Å—Ç—Ä–æ–∫–∏\n"
            f"‚Ä¢ –°—Ç—Ä–æ–∫–∞ –∏–∑ –ª–æ–≥–∞",
            reply_markup=build_main_menu_keyboard()
        )

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}", exc_info=True)
        await message.answer(
            f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
        )
    finally:
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        await state.clear()


@router.message(UploadStates.waiting_for_file)
async def handle_invalid_upload(message: Message) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –æ–∂–∏–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞."""

    await message.answer(
        "‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏ (txt, log –∏–ª–∏ zip).\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫—É '–ó–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–≥–∏' –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞."
    )

