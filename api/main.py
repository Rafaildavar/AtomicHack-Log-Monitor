"""FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–ª–ª–µ–≥–∏, —á—Ç–æ –∏ Telegram –±–æ—Ç.
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Excel –æ—Ç—á–µ—Ç—ã –≤ —Ç–æ–º –∂–µ —Ñ–æ—Ä–º–∞—Ç–µ, —á—Ç–æ –∏ –¥–ª—è –∑–∞—â–∏—Ç—ã.
"""

import logging
import os
import tempfile
from typing import Optional

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Form
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import plotly.graph_objects as go

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É –∫–æ–ª–ª–µ–≥–∏ –∏–∑ core
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from core.services.ml_analyzer import MLLogAnalyzer
from core.services.log_parser import LogParser
from core.services.report_generator import ReportGenerator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="AtomicHack Log Monitor API",
    description="API –¥–ª—è ML-–∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ Telegram –±–æ—Ç.",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Ä–≤–∏—Å—ã (–ª–æ–≥–∏–∫–∞ –∫–æ–ª–ª–µ–≥–∏)
ml_analyzer = MLLogAnalyzer(similarity_threshold=0.7)
log_parser = LogParser()
report_generator = ReportGenerator()

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
REPORTS_DIR = os.path.join(os.path.dirname(__file__), 'reports')
os.makedirs(REPORTS_DIR, exist_ok=True)

# –ó–∞–≥—Ä—É–∂–∞–µ–º ML –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤)
logger.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ ML –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ API...")
ml_analyzer._load_model()
logger.info("‚úÖ ML –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∞–Ω–∞–ª–∏–∑–∞–º")


async def _save_upload_file(upload_file: UploadFile, destination_path: str) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫ –ø–æ —á–∞—Å—Ç—è–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞–Ω–∏–º–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É.
    """
    os.makedirs(os.path.dirname(destination_path), exist_ok=True)
    # –ß–∏—Ç–∞–µ–º –∏ –ø–∏—à–µ–º –ø–æ 1 –ú–ë
    chunk_size_bytes = 1024 * 1024
    with open(destination_path, 'wb') as out_file:
        while True:
            chunk = await upload_file.read(chunk_size_bytes)
            if not chunk:
                break
            out_file.write(chunk)
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —É–∫–∞–∑–∞—Ç–µ–ª—å, –µ—Å–ª–∏ —Ñ–∞–π–ª –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ —á–∏—Ç–∞—Ç—å
    try:
        await upload_file.seek(0)
    except Exception:
        pass
    return destination_path


def generate_log_visualization(logs_df: pd.DataFrame) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–æ–≥–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
    
    Args:
        logs_df: DataFrame —Å –ª–æ–≥–∞–º–∏ (columns: datetime, level, text, filename, line_number)
    
    Returns:
        HTML —Å—Ç—Ä–æ–∫–∞ —Å –≥—Ä–∞—Ñ–∏–∫–æ–º
    """
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        df = logs_df.copy()
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        if 'datetime' not in df.columns or 'level' not in df.columns:
            logger.warning("–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º")
            return "<div>–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞</div>"
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º datetime –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')
        df = df.dropna(subset=['datetime'])
        
        if df.empty:
            return "<div>–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞</div>"
        
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é
        level_counts = df['level'].value_counts().to_dict()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∏ —Ü–≤–µ—Ç–∞
        level_order = ["INFO", "WARNING", "ERROR"]
        color_map = {
            "INFO": "#87CEEB",      # Light sky blue
            "WARNING": "#FFD700",    # Gold
            "ERROR": "#FF6347"       # Tomato
        }
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —É—Ä–æ–≤–Ω–∏
        df = df[df['level'].isin(level_order)]
        
        if df.empty:
            return "<div>–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è</div>"
        
        # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö —É–º–µ–Ω—å—à–∏–º –≤—ã–±–æ—Ä–∫—É, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–¥—É–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
        max_points = 20000
        if len(df) > max_points:
            df = df.sample(n=max_points, random_state=42)

        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π scatter –≥—Ä–∞—Ñ–∏–∫
        fig = go.Figure()
        
        for level in level_order:
            level_data = df[df['level'] == level]
            if not level_data.empty:
                count = len(level_data)
                fig.add_trace(go.Scatter(
                    x=level_data['datetime'],
                    y=[level] * count,
                    mode='markers',
                    name=f"{level} ({count})",
                    marker=dict(
                        size=10,
                        color=color_map.get(level, "#999999"),
                        opacity=0.8,
                        line=dict(width=1, color="white")
                    ),
                    text=level_data['text'].astype(str),
                    hovertemplate='<b>%{y}</b><br>–í—Ä–µ–º—è: %{x}<br>–°–æ–æ–±—â–µ–Ω–∏–µ: %{text}<extra></extra>'
                ))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º layout
        fig.update_layout(
            title="üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏",
            xaxis_title="–í—Ä–µ–º—è",
            yaxis_title="–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è",
            template="plotly_dark",
            height=400,
            hovermode='closest',
            yaxis=dict(categoryorder="array", categoryarray=level_order),
            plot_bgcolor="#0a0e27",
            paper_bgcolor="#0a0e27",
            font=dict(color="white", family="Arial, sans-serif")
        )
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º HTML
        html = fig.to_html(include_plotlyjs='cdn', config={'responsive': True})
        return html
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
        return f"<div style='color: red;'>–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}</div>"


def generate_anomaly_graph(results_df: pd.DataFrame, anomalies_df: pd.DataFrame) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∞–Ω–æ–º–∞–ª–∏—è–º–∏ –∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏.
    
    Args:
        results_df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        anomalies_df: DataFrame —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º –∞–Ω–æ–º–∞–ª–∏–π
    
    Returns:
        HTML —Å—Ç—Ä–æ–∫–∞ —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º –≥—Ä–∞—Ñ–æ–º
    """
    try:
        import networkx as nx
        from pyvis.network import Network
        import tempfile
        
        if results_df.empty:
            return "<div>–ù–µ—Ç –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞</div>"
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ
        G = nx.Graph()
        
        # –ë–µ—Ä–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã –∞–Ω–æ–º–∞–ª–∏—è-–ø—Ä–æ–±–ª–µ–º–∞
        for _, row in results_df.iterrows():
            anom_id = int(row.get('ID –∞–Ω–æ–º–∞–ª–∏–∏', -1))
            prob_id = int(row.get('ID –ø—Ä–æ–±–ª–µ–º—ã', -1))
            
            if anom_id == -1 or prob_id == -1:
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –∏ –ø—Ä–æ–±–ª–µ–º—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è
            try:
                anom_matches = anomalies_df[anomalies_df['ID –∞–Ω–æ–º–∞–ª–∏–∏'].astype(int) == anom_id]
                prob_matches = anomalies_df[anomalies_df['ID –ø—Ä–æ–±–ª–µ–º—ã'].astype(int) == prob_id]
                
                anom_text = anom_matches['–ê–Ω–æ–º–∞–ª–∏—è'].values[0] if len(anom_matches) > 0 else 'Unknown'
                prob_text = prob_matches['–ü—Ä–æ–±–ª–µ–º–∞'].values[0] if len(prob_matches) > 0 else 'Unknown'
            except Exception:
                anom_text = 'Unknown'
                prob_text = 'Unknown'
            
            anom_label = f"Anom {anom_id}: {str(anom_text)[:30]}..."
            prob_label = f"Prob {prob_id}: {str(prob_text)[:30]}..."
            
            # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã –∏ —Ä–µ–±—Ä–∞
            G.add_node(anom_label, node_type="anomaly")
            G.add_node(prob_label, node_type="problem")
            G.add_edge(anom_label, prob_label)
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é PyVis
        net = Network(
            height="500px",
            width="100%",
            bgcolor="#0a0e27",
            font_color="white",
            directed=False
        )
        
        net.from_nx(G)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞
        net.set_options("""
        {
          "physics": {
            "enabled": true,
            "barnesHut": {
              "gravitationalConstant": -2000,
              "centralGravity": 0.3,
              "springLength": 150,
              "springConstant": 0.04
            }
          },
          "nodes": {
            "font": {"size": 16, "face": "Arial"},
            "scaling": {"min": 20, "max": 40}
          },
          "edges": {
            "color": {"color": "#00D4FF", "opacity": 0.6},
            "smooth": true
          }
        }
        """)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, dir=REPORTS_DIR) as f:
            temp_path = f.name
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ
        net.save_graph(temp_path)
        
        # –ß–∏—Ç–∞–µ–º HTML
        with open(temp_path, 'r', encoding='utf-8') as f:
            html = f.read()
        
        # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª
        try:
            os.remove(temp_path)
        except:
            pass
        
        return html
        
    except ImportError:
        logger.error("–û—à–∏–±–∫–∞: networkx –∏–ª–∏ pyvis –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        return "<div style='color: orange;'>GraphX libraries not available</div>"
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞: {e}")
        return f"<div style='color: red;'>–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞: {str(e)}</div>"


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç."""
    return {
        "message": "AtomicHack Log Monitor API",
        "version": "1.0.0",
        "team": "Black Lotus",
        "docs": "/docs",
        "status": "running"
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API."""
    return {
        "status": "healthy",
        "ml_model": "loaded" if ml_analyzer.model is not None else "not_loaded",
        "services": {
            "ml_analyzer": "ready",
            "log_parser": "ready",
            "report_generator": "ready"
        }
    }


@app.post("/api/v1/analyze")
async def analyze_logs(
    log_file: UploadFile = File(..., description="–§–∞–π–ª —Å –ª–æ–≥–∞–º–∏ (.txt, .log, .zip)"),
    anomalies_file: Optional[UploadFile] = File(None, description="–°–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π (anomalies_problems.csv)"),
    threshold: str = Form("0.7")
):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML (–ª–æ–≥–∏–∫–∞ –∫–æ–ª–ª–µ–≥–∏).
    
    **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞, —á—Ç–æ –∏ Telegram –±–æ—Ç.**
    
    Args:
        log_file: –§–∞–π–ª —Å –ª–æ–≥–∞–º–∏ (txt, log –∏–ª–∏ zip)
        anomalies_file: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π)
        threshold: –ü–æ—Ä–æ–≥ similarity –¥–ª—è ML-–º–æ–¥–µ–ª–∏ (0.0-1.0)
    
    Returns:
        JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Å—Å—ã–ª–∫–æ–π –Ω–∞ Excel –æ—Ç—á–µ—Ç
    """
    temp_dir = tempfile.mkdtemp()
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º threshold –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤ —á–∏—Å–ª–æ
        try:
            threshold_float = float(threshold)
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ threshold –≤ –¥–æ–ø—É—Å—Ç–∏–º–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            threshold_float = max(0.0, min(1.0, threshold_float))
        except (ValueError, TypeError):
            threshold_float = 0.7
            logger.warning(f"–ù–µ–≤–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ threshold: {threshold}, –∏—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ 0.7")
        
        logger.info(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑: {log_file.filename}")
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {threshold_float}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏ (–ø–æ—à—Ç—É—á–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å OOM)
        log_file_path = os.path.join(temp_dir, log_file.filename)
        await _save_upload_file(log_file, log_file_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–π–ª—ã —Å –ª–æ–≥–∞–º–∏
        if log_file.filename.endswith('.zip'):
            logger.info("–ò–∑–≤–ª–µ–∫–∞–µ–º ZIP –∞—Ä—Ö–∏–≤")
            log_files = log_parser.extract_zip(log_file_path, temp_dir)
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ª–æ–≥-—Ñ–∞–π–ª—ã (–Ω–µ CSV)
            log_files = [f for f in log_files if not f.endswith('anomalies_problems.csv')]
        else:
            log_files = [log_file_path]
        
        # –ü–∞—Ä—Å–∏–º –ª–æ–≥–∏ (–ª–æ–≥–∏–∫–∞ –∫–æ–ª–ª–µ–≥–∏)
        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {len(log_files)} —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤")
        logs_df = log_parser.parse_log_files(log_files)
        
        if logs_df.empty:
            raise HTTPException(status_code=400, detail="–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –ª–æ–≥–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.")
        
        logger.info(f"–†–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(logs_df)} —Å—Ç—Ä–æ–∫ –ª–æ–≥–æ–≤")
        
        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        basic_analysis = log_parser.analyze_logs_basic(logs_df)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π
        if anomalies_file:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å: {anomalies_file.filename}")
            anomalies_path = os.path.join(temp_dir, anomalies_file.filename)
            await _save_upload_file(anomalies_file, anomalies_path)
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ ZIP –∞—Ä—Ö–∏–≤
            if log_file.filename.lower().endswith('.zip'):
                extracted_anomalies = [f for f in log_parser.extract_zip(log_file_path, temp_dir) 
                                      if f.endswith('anomalies_problems.csv')]
                
                if extracted_anomalies:
                    logger.info(f"–ù–∞–π–¥–µ–Ω —Å–ª–æ–≤–∞—Ä—å –≤ ZIP: {extracted_anomalies[0]}")
                    anomalies_path = extracted_anomalies[0]
                else:
                    anomalies_path = None
            else:
                anomalies_path = None
            
            # –ï—Å–ª–∏ —Å–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
            if not anomalies_path:
                default_anomalies = os.path.join(
                    os.path.dirname(__file__), 
                    '..', 'src', 'bot', 'services', 'anomalies_problems.csv'
                )
                if os.path.exists(default_anomalies):
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π")
                    anomalies_path = default_anomalies
                else:
                    raise HTTPException(
                        status_code=400, 
                        detail="–°–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª anomalies_problems.csv"
                    )
        
        # –ß–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π
        anomalies_df = pd.read_csv(anomalies_path, sep=';', encoding='utf-8')
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(anomalies_df)} –∞–Ω–æ–º–∞–ª–∏–π –∏–∑ —Å–ª–æ–≤–∞—Ä—è")
        
        # ML-–∞–Ω–∞–ª–∏–∑ (–õ–û–ì–ò–ö–ê –ö–û–õ–õ–ï–ì–ò –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô)
        ml_analyzer.similarity_threshold = threshold_float
        logger.info(f"–ó–∞–ø—É—Å–∫ ML-–∞–Ω–∞–ª–∏–∑–∞ —Å –ø–æ—Ä–æ–≥–æ–º {threshold_float}")
        results_df = ml_analyzer.analyze_logs_with_ml(logs_df, anomalies_df)
        
        logger.info(f"ML-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(results_df)} –ø—Ä–æ–±–ª–µ–º")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        summary = ml_analyzer.get_analysis_summary(results_df)
        
        # –°–æ–∑–¥–∞–µ–º Excel –æ—Ç—á–µ—Ç (–¢–û–ß–ù–û –¢–ê–ö –ñ–ï –ö–ê–ö –î–õ–Ø –ó–ê–©–ò–¢–´)
        excel_report_path = None
        if not results_df.empty:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ '–°—Ü–µ–Ω–∞—Ä–∏–π' –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å report_generator
            results_df_with_scenario = results_df.copy()
            results_df_with_scenario['–°—Ü–µ–Ω–∞—Ä–∏–π'] = 1  # ID —Å—Ü–µ–Ω–∞—Ä–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DataFrame –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è report_generator (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –±–æ—Ç–µ)
            analysis_results = [{
                'results': results_df_with_scenario.to_dict('records')
            }]
            
            excel_report_path = os.path.join(temp_dir, f"analysis_report_{log_file.filename}.xlsx")
            excel_report_path = report_generator.create_excel_report(
                analysis_results, 
                excel_report_path
            )
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –æ—Ç—á–µ—Ç –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–π
            import shutil
            final_excel_path = os.path.join(REPORTS_DIR, os.path.basename(excel_report_path))
            try:
                shutil.copy2(excel_report_path, final_excel_path)
                excel_report_path = final_excel_path
            except Exception:
                # –ï—Å–ª–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—É—Ç—å, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å Excel –≤ reports/, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª")
            logger.info(f"Excel –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {excel_report_path}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç–∫–ª—é—á–∞–µ–º —Ç—è–∂–µ–ª—ã–µ HTML –≥—Ä–∞—Ñ—ã –≤ –æ—Ç–≤–µ—Ç–µ
        enable_log_vis = not logs_df.empty and len(logs_df) <= 100_000
        enable_anomaly_graph = not results_df.empty and len(results_df) <= 500

        response = {
            "status": "success",
            "analysis": {
                "basic_stats": basic_analysis,
                "ml_results": summary,
                "threshold_used": threshold_float
            },
            "results": results_df.to_dict('records') if not results_df.empty else [],
            "excel_report": f"/api/v1/download/{os.path.basename(excel_report_path)}" if excel_report_path else None,
            "log_visualization": generate_log_visualization(logs_df) if enable_log_vis else None,
            "anomaly_graph": generate_anomaly_graph(results_df, anomalies_df) if enable_anomaly_graph else None
        }
        
        return response
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/download/{filename}")
async def download_report(filename: str):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Excel –æ—Ç—á–µ—Ç.
    
    **–§–æ—Ä–º–∞—Ç Excel —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ –¥–ª—è –∑–∞—â–∏—Ç—ã –Ω–∞ —Ö–∞–∫–∞—Ç–æ–Ω–µ.**
    """
    # –ò—â–µ–º —Ñ–∞–π–ª –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ reports
    file_path = os.path.join(REPORTS_DIR, filename)
    
    if not os.path.exists(file_path):
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ reports, –∏—â–µ–º –≤ temp
        file_path = os.path.join(tempfile.gettempdir(), filename)
    
    if not os.path.exists(file_path):
        logger.warning(f"Excel —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filename}")
        raise HTTPException(status_code=404, detail=f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ Excel –æ—Ç—á–µ—Ç–∞: {file_path}")
    return FileResponse(
        path=file_path,
        filename=filename,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )


@app.get("/api/v1/anomalies/default")
async def get_default_anomalies():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∞–Ω–æ–º–∞–ª–∏–π."""
    default_anomalies = os.path.join(
        os.path.dirname(__file__), 
        '..', 'src', 'bot', 'services', 'anomalies_problems.csv'
    )
    
    if not os.path.exists(default_anomalies):
        raise HTTPException(status_code=404, detail="–î–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    return FileResponse(
        path=default_anomalies,
        filename="anomalies_problems.csv",
        media_type="text/csv"
    )


if __name__ == "__main__":
    import uvicorn
    
    logger.info("–ó–∞–ø—É—Å–∫ AtomicHack Log Monitor API")
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–ª–ª–µ–≥–∏ –∏–∑ core/")
    logger.info("–§–æ—Ä–º–∞—Ç Excel –æ—Ç—á–µ—Ç–æ–≤ - –∫–∞–∫ –¥–ª—è –∑–∞—â–∏—Ç—ã —Ö–∞–∫–∞—Ç–æ–Ω–∞")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )

